\documentclass[twocolumn, a4paper]{hcresume}

% 日本語処理のためのパッケージ
\usepackage{otf}
\usepackage{hyperref}
\usepackage[dvipdfmx]{pxjahyper}
\usepackage[dvipdfmx]{graphicx}
\usepackage{bm}
\usepackage{amsmath}
\usepackage{txfonts}
\usepackage{color}
\usepackage{BoldGothic4fig}
\usepackage{subcaption}
\usepackage{listings}
\lstset{
  basicstyle=\ttfamily\tiny,
  numbers=none,
  frame=none,
  breaklines=true,
  breakindent=0pt,
  postbreak=,
  prebreak=
}

\hcheader{MIプログラム 卒研中間発表会}
\title{\bf 精緻化した受験者プロフィールを用いた \\ 大規模言語モデルに基づく仮想受験者}
\author{2020029　山羽　亨}
\supervisor{指導教員 宇都 雅輝 准教授}

\begin{document}
\maketitle
\pagestyle{empty}
\thispagestyle{empty}
\section{はじめに}

近年ニーズが高まっている個別適応的な学習支援やCBT(Computer Based Testing)などに基づく高度なテスト運用においては，難易度や識別力などの特性が既知のテスト問題（以降，項目と呼ぶ）が重要な役割を果たす．
例えば，知的学習支援システムでは，学習者の能力にあった難易度の項目を適応的に出題する機能が重要要件の一つとなるが，この実現のためには各項目の難易度が既知である必要がある．
またCBTでは，異なる項目群で構成されるにも関わらず統計的な性質が類似したテスト版を多数構成して活用することで，
高精度な能力測定を担保しつつ不正防止や試験結果の年度間比較などが実現されるが，このためにも各項目の特性が既知である必要がある．
そのため，これらの場面では，予め目標の母集団から受験者や学習者(以降，受験者に統一)を集めて関心下の項目を出題し，得られた反応データから項目特性を推定する「事前テスト」が一般に行われる．
しかし，事前テストの実施には多大なコストがかかる上に項目内容の漏洩リスクも生じる．

この問題を解決するために，自然言語処理技術を用いて問題文から項目特性を推定する方法論が研究されている\cite{uto2024question,benedetto-etal-2024-using}．
古典的な方法論としては，問題文を入力として項目特性値を出力する機械学習モデルを教師あり学習の枠組みで構築する手法が研究されてきた．しかしこの方法では，モデル訓練のために特性値が既知の項目を大量に用意する必要があり，そのような項目集合を作るために，目的の学習領域に関する大量の項目を目標とする受験者集団に出題して事前に解かせる必要があった．このような方法論は，大規模かつ長期間運用されてきた試験や学習支援システム以外では実現が困難であり，活用可能な場面が限定的であった．これとは異なる方法論として，近年では，自然言語処理技術の一つである質問応答システムを応用して人間受験者の項目反応を模倣する「仮想受験者」と呼ぶ技術が提案されている．
例えば，性能の異なる多様な大規模言語モデル(Large Language Models)を用いて構築した質問応答システム群を仮想受験者集団とみなす手法が提案されている．
この方法では，人間受験者の項目反応や特性が既知の項目データセットを利用せずに，仮想受験者を構築できるが，仮想受験者と人間受験者の反応傾向が一致する保証はない．
他方で，過去の項目群に対する人間受験者の反応データを用いて質問応答システムを訓練することで仮想受験者を構築する手法も提案されている．
この方法では，人間受験者の項目反応を模倣させやすい一方で，古典的な方法論と同様に，人間受験者の反応データの収集に多大なコストを要する．

これらの問題に対処できる仮想受験者の実現方法の一つとして，Benedetto et al.\cite{benedetto-etal-2024-using}は，LLMへのプロンプトとして，問題文と共に想定する受験者の特徴を与えることで，ゼロショットで受験者反応を模倣する手法を提案している．しかし，従来手法では，プロンプトに与える受験者の特徴が少数段階の能力レベルの指示のみに留まっており，細やかな知識状態の違いを模倣できていない．さらに，従来手法では，LLMによっては，適切にスキルレベルの調整ができないことを示している．これらはいずれも，プロンプトで与える受験者のプロフィールが荒いことに起因すると考えられる．

これらの問題を解決するために，本研究では，個別の受験者がどのような知識・技能を持っているかの詳細な定義を受験者プロフィールとして記述し，その情報をプロンプトとして与えることで，より精密に目標の受験者の反応を予測できる擬似受験者手法を提案する．具体的には，英語の読解力テストを対象に，ヨーロッパ言語共通参照枠（Common
European Framework of Reference for Languages: CEFR）に基づくA1からC2の各スキルレベルに対応する受験者プロフィールをCEFR Can-Do Statementを参考に設計して，プロンプトに含める．

本研究では，実データ実験を通して提案モデルの有効性を示す．

\section{関連研究}

近年，大規模言語モデル（LLM）を用いて異なるスキルレベルの学習者の試験問題への反応を模擬する手法が提案されている\cite{benedetto-etal-2024-using}．この手法では，GPT-3.5やGPT-4などのLLMに対して特定のスキルレベル（1〜5の5段階）の学習者として振る舞うよう指示するプロンプトを設計し，多肢選択問題への回答を生成させる．LLMへの入力はシステムプロンプト（模擬する学習者レベルと出力形式の指定），試験問題のテキスト，選択肢の一覧，読解問題の場合は読解パッセージから構成される．LLMに期待する出力は問題の難易度レベル，選択した回答の説明（そのスキルレベルの学習者が辿る思考過程や誤解を含む），選択した回答のインデックスを含むJSON形式である．

% \textbf{Output (in JSON):}
% \begin{verbatim}
% {
%   "question_level": <question_level>,
%   "answer_explanation": "<answer_explanation>",
%   "index": <index>
% }
% \end{verbatim}

先行研究では，プロンプトエンジニアリングによる最適化を通じて，LLMが模擬する学習者レベルごとに現実的な正答率分布を再現できるよう調整が行われた．プロンプトの最適化では，5つのレベルを模擬した際の正答率$L = (a_1, a_2, a_3, a_4, a_5)$と理想的な正答率曲線$I = (0.0, 0.25, 0.5, 0.75, 1.0)$間のピアソン相関係数$\rho_{L,I}$を算出し，非単調性に対するペナルティ$P$を以下の式で計算する．

\begin{equation}
P = \sum_{i=1}^{4} \sqrt{|a_{i+1} - a_i|} \cdot I(a_{i+1} < a_i)
\end{equation}

ここで，$I(a_{i+1} < a_i)$は指示関数である．最終的な評価指標$M$は相関係数からペナルティを差し引いた値として定義される．

\begin{equation}
M = \rho_{L,I} - P
\end{equation}

仮想受験者モデルの妥当性は単調増加性（模擬するスキルレベルの上昇に伴う正答率の単調増加），難易度依存性（同一スキルレベルでより困難な問題での正答率低下），仮想プレテストの妥当性（仮想受験者による問題困難度推定と実際の受験者による困難度との相関）の観点から評価される．

実験では，GPT-3.5を用いた場合に模擬レベルの増加に伴う正答率の単調増加が確認されたが，プロンプトエンジニアリングを行ったGPT-3.5の特定バージョンでの成果は他のLLMには十分に汎化しないことが明らかになった．この汎化性の不足は，プロンプトで与える受験者プロフィールが荒いことに起因すると考えられる．

\section{提案手法}

従来手法では，仮想受験者のスキルレベルが1から5の数値のみで記述されており，具体的な知識状態が不明確であるため，人間受験者の項目反応との比較が困難である．また，GPT-4など高性能なモデルでは，プロンプト最適化の効果が限定的で，低スキルレベルの受験者反応を適切に模倣できていない．

本研究では，CEFR Global ScaleのA2〜C2に対応する詳細な受験者プロフィールをプロンプトに追加することで，高性能なLLMでも妥当な受験者反応を模倣できる手法を提案する．具体的には，CEFR Can-Do Descriptorsを参考に，読解問題の正答に必要な知識・技能を各スキルレベルごとに整理し，各レベルに応じた語彙制限とともにプロンプトに含める．

\section{実験}

\textbf{使用データセット}　再現実験では，RACEデータセットを用いた．RACEは英語読解問題の多肢選択式問題データセットで，各問題には3つのレベル（middle, high, college）が割り当てられており，middleが最低難易度，collegeが最高難易度とされている．先行研究と同様に，テスト分割から層化サンプリングによって得られた150問の縮小セット（各レベル50問ずつ）を使用した．

\textbf{実験手順}　先行研究で用いられたプログラムを一部改変し，以下の手順で実施した．まず，gpt-3.5-turbo-1106を用いて先行研究のプロンプトによる再現実験を行い，LLMの回答と正答から正答率を算出した．次に，より高性能なgpt-4o-miniで同様の実験を実施し，低スキルレベル学習者の正答率が高くなる問題と単調増加性の破綻を確認した．さらに，CEFR Can-Do Statementを追加したプロンプトを用いてgpt-4o-miniで実験を行った．

\textbf{使用プロンプト}　再現実験では先行研究のプロンプトを使用し，提案手法では以下に示すようにCEFR Can-Do Statementの詳細な受験者プロフィールを追加したプロンプトを用いた．

\noindent\rule{\linewidth}{0.4pt}
\begin{lstlisting}[caption={CEFR Can-Do Statementを追加したプロンプト}]
You will be shown a multiple choice question from an English reading comprehension exam, and the questions in the exam have difficulty levels on a scale from one (very easy) to five (very difficult).You must assign a difficulty level to the given multiple choice question, and select the answer choice that a student of CEFR level {student_level} would pick.A student of CEFR level {student_level} {get_cefr_levels_detailed_description(student_level)}.Provide only a JSON file with the following structure:{{"question level": "difficulty level of the question", "answer explanation": "the list of steps that the students of CEFR level {student_level} would follow to select the answer, including the misconceptions that might cause them to make mistakes", "index": "integer index of the answer chosen by a student of CEFR level {student_level}"}}
\end{lstlisting}
\noindent\rule{\linewidth}{0.4pt}

\textbf{実験結果}　図\ref{fig:gpt-3-5}に示すように，gpt-3.5-turbo-1106では，CEFRレベルの上昇に伴い正答率が単調増加する傾向が確認されたが，C1レベルでB2レベルより正答率が低くなり，単調増加性が完全には満たされなかった．図\ref{fig:gpt-4o-mini}に示すように，gpt-4o-miniでは，C2レベルで他スキルレベルより大幅な正答率低下が観測された．図\ref{fig:comparison}の比較では，両モデルとも異なるレベルで単調増加性の破綻が確認され，先行研究で指摘された最適化プロンプトの他LLMモデルへの汎化性不足が再現された．

\begin{figure}[h]
\centering
\begin{minipage}[b]{0.49\columnwidth}
    \centering
    \includegraphics[width=0.9\columnwidth]{img/gpt_3_5_1106/60_accuracy_per_difficulty_with_different_roleplayed_levels.eps}
    \subcaption{難易度別正答率}
    \label{fig:gpt-3-5-1}
    \vspace{3mm}
\end{minipage}
\begin{minipage}[b]{0.49\columnwidth}
    \centering
    \includegraphics[width=0.9\columnwidth]{img/gpt_3_5_1106/60_accuracy_per_roleplayed_level_on_different_difficulty_levels.eps}
    \subcaption{スキルレベル別正答率}
    \label{fig:gpt-3-5-2}
    \vspace{3mm}
\end{minipage}
\caption{GPT-3.5-turbo-1106の実験結果}
\label{fig:gpt-3-5}
\end{figure}

\begin{figure}[h]
\centering
\begin{minipage}[b]{0.49\columnwidth}
    \centering
    \includegraphics[width=0.9\columnwidth]{img/gpt_4o_mini/68_accuracy_per_difficulty_with_different_roleplayed_levels.eps}
    \subcaption{難易度別正答率}
    \label{fig:gpt-4o-mini-1}
    \vspace{3mm}
\end{minipage}
\begin{minipage}[b]{0.49\columnwidth}
    \centering
    \includegraphics[width=0.9\columnwidth]{img/gpt_4o_mini/68_accuracy_per_roleplayed_level_on_different_difficulty_levels.eps}
    \subcaption{スキルレベル別正答率}
    \label{fig:gpt-4o-mini-2}
    \vspace{3mm}
\end{minipage}
\caption{GPT-4o-miniの実験結果}
\label{fig:gpt-4o-mini}
\end{figure}

\begin{figure}[h]
\centering
\begin{minipage}[b]{0.49\columnwidth}
    \centering
    \includegraphics[width=0.9\columnwidth]{img/gpt_3_5_1106/60_accuracy_per_roleplayed_level.eps}
    \subcaption{gpt-3.5-turbo-1106}
    \label{fig:comparison-gpt35}
    \vspace{3mm}
\end{minipage}
\begin{minipage}[b]{0.49\columnwidth}
    \centering
    \includegraphics[width=0.9\columnwidth]{img/gpt_4o_mini/68_accuracy_per_roleplayed_level.eps}
    \subcaption{gpt-4o-mini}
    \label{fig:comparison-gpt4o}
    \vspace{3mm}
\end{minipage}
\caption{両モデルのスキルレベル別正答率比較}
\label{fig:comparison}
\end{figure}

% \noindent\rule{\linewidth}{0.4pt} % 横棒（上）
% \begin{lstlisting}[caption={先行研究で用いられたプロンプト}]
% You will be shown a multiple choice question from an English reading comprehension exam, and the questions in the exam have difficulty levels on a scale from one (very easy) to five (very difficult).
% You must assign a difficulty level to the given multiple choice question, and select the answer choice that a student of CEFR level {student_level} would pick.
% A student of CEFR level {student_level} {get_cefr_levels_description(student_level)}.
% Provide only a JSON file with the following structure:
% {{"question level": "difficulty level of the question", "answer explanation": "the list of steps that the students of CEFR level {student_level} would follow to select the answer, including the misconceptions that might cause them to make mistakes", "index": "integer index of the answer chosen by a student of CEFR level {student_level}"}}
% \end{lstlisting}
% \noindent\rule{\linewidth}{0.4pt} % 横棒（下）




\section{まとめ}

本研究では，LLMを用いた仮想受験者モデルの精度向上のため，CEFR Can-Do Statementに基づく詳細な受験者プロフィールをプロンプトに追加する手法を提案した．再現実験により，従来手法では最適化されたプロンプトが他のLLMモデルに汎化しない問題を確認し，より精緻な受験者プロフィールによる改善の必要性を示した．今後は提案手法の実装と有効性検証，他データセットでの汎化性確認を行う予定である． 

{\small
\begin{thebibliography}{99}
% \bibitem{ueno2018irt} 植野真臣, 宮澤芳光, ``IRT-Based Adaptive Hints to Scaffold Learning in Programming", IEEE Transactions on Learning Technologies, Vol.11, No.4, pp.415--428, 2018.
% \bibitem{baker2004item} F. B. Baker, S.-H. Kim, \textit{Item Response Theory: Parameter Estimation Techniques}, CRC Press, 2004.
% \bibitem{van1996handbook} W. J. van der Linden, R. K. Hambleton, \textit{Handbook of Modern Item Response Theory}, Springer New York, 1996.
% \bibitem{kolen2014test} M. J. Kolen, R. L. Brennan, \textit{Test Equating, Scaling, and Linking}, Springer New York, 2014.
% \bibitem{devlin2018bert} J. Devlin, M.-W. Chang, K. Lee, K. Toutanova, ``BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", arXiv preprint arXiv:1810.04805, 2018.
% \bibitem{benedetto2023quantitative} L. Benedetto, ``A quantitative study of NLP approaches to question difficulty estimation", Proceedings of the International Conference on Artificial Intelligence in Education, pp.428--434, 2023.
% \bibitem{gao2019difficulty} Y. Gao, L. Bing, W. Chen, M. Lyu, I. King, ``Difficulty Controllable Generation of Reading Comprehension Questions", Proc. Twenty-Eighth International Joint Conf. Artificial Intelligence, pp.4968--4974, 2019.
% \bibitem{byrd2022predicting} M. Byrd, S. Srivastava, ``Predicting difficulty and discrimination of natural language questions", Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics, pp.119--130, 2022.
\bibitem{uto2024question} M. Uto, Y. Tomikawa, A. Suzuki, ``Question Difficulty Prediction Based on Virtual Test-Takers and Item Response Theory", AIED'24: Workshop on Automatic Evaluation of Learning and Assessment Content, pp.1--11, 2024.
\bibitem{benedetto-etal-2024-using} L. Benedetto, G. Aradelli, A. Donvito, A. Lucchetti, A. Cappelli, P. Buttery, ``Using LLMs to simulate students' responses to exam questions", Findings of the Association for Computational Linguistics: EMNLP 2024, pp.11351--11368, 2024.
% \bibitem{lai2017race} G. Lai, Q. Xie, H. Liu, Y. Yang, E. Hovy, ``RACE: Large-scale ReAding Comprehension Dataset From Examinations", Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pp.785--794, 2017.
% \bibitem{liang2019new} Y. Liang, J. Li, J. Yin, ``A new multi-choice reading comprehension dataset for curriculum learning", Asian Conference on Machine Learning, pp.742--757, 2019.
% \bibitem{maeda2025field} H. Maeda, ``Field-testing multiple-choice questions with AI examinees: English grammar items", Educational and Psychological Measurement, Vol.85, No.2, pp.221--244, 2025.
% \bibitem{council2001} Council of Europe, \textit{Common European Framework of Reference for Languages: Learning, Teaching, Assessment}, Cambridge University Press, 2001.
\end{thebibliography}
}
\end{document}
