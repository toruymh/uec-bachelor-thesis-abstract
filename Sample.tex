\documentclass[twocolumn, a4paper]{hcresume}

% 日本語処理のためのパッケージ
\usepackage{otf}
\usepackage{hyperref}
\usepackage[dvipdfmx]{pxjahyper}
\usepackage[dvipdfmx]{graphicx}
\usepackage{bm}
\usepackage{amsmath}
\usepackage{txfonts}
\usepackage{color}
\usepackage{BoldGothic4fig}
\usepackage{subcaption}
\usepackage{float}
\usepackage{here}
\usepackage{listings}
\lstset{
  basicstyle=\ttfamily\tiny,
  numbers=none,
  frame=none,
  breaklines=true,
  breakindent=0pt,
  postbreak=,
  prebreak=
}
\newfloat{lstfloat}{htbp}{lop}
\floatname{lstfloat}{Listing}
\def\lstfloatautorefname{Listing}


\hcheader{MIプログラム 卒業研究発表会}
\title{\bf 精緻化した受験者プロフィールを用いた \\ 大規模言語モデルに基づく仮想受験者}
\author{2020029　山羽　亨}
\supervisor{指導教員 宇都 雅輝 准教授}

\begin{document}
\maketitle
\pagestyle{empty}
\thispagestyle{empty}
\section{はじめに}

近年ニーズが高まっている個別適応的な学習支援やCBT(Computer Based Testing)などに基づく高度なテスト運用においては，難易度や識別力などの特性が既知のテスト問題（以降，項目と呼ぶ）が重要な役割を果たす．
例えば，知的学習支援システムでは，学習者の能力にあった難易度の項目を適応的に出題する機能が重要要件の一つとなるが，この実現のためには各項目の難易度が既知である必要がある．
またCBTでは，異なる項目群で構成されるにも関わらず統計的な性質が類似したテスト版を多数構成して活用することで，
高精度な能力測定を担保しつつ不正防止や試験結果の年度間比較などが実現されるが，このためにも各項目の特性が既知である必要がある．
そのため，これらの場面では，予め目標の母集団から受験者や学習者(以降，受験者に統一)を集めて関心下の項目を出題し，得られた反応データから項目特性を推定する「事前テスト」が一般に行われる．
しかし，事前テストの実施には多大なコストがかかる上に項目内容の漏洩リスクも生じる．

この問題を解決するために，自然言語処理技術を用いて問題文から項目特性を推定する方法論が研究されている．
古典的な方法論としては，問題文を入力として項目特性値を出力する機械学習モデルを教師あり学習の枠組みで構築する手法が研究されてきた\cite{devlin2018bert,benedetto2023quantitative}．
しかしこの方法では，モデル訓練のために特性値が既知の項目を大量に用意する必要があり，そのような項目集合を作るために，目的の学習領域に関する大量の項目を目標とする受験者集団に出題して事前に解かせる必要があった．このような方法論は，大規模かつ長期間運用されてきた試験や学習支援システム以外では実現が困難であり，活用可能な場面が限定的であった

これとは異なる方法論として，近年では，自然言語処理技術の一つである質問応答システムを応用して人間受験者の項目反応を模倣する「仮想受験者」と呼ぶ技術が提案されている\cite{tomikawa2024adaptive,uto2024question,benedetto-etal-2024-using}．
例えば，性能の異なる多様な大規模言語モデル(Large Language Models)を用いて構築した質問応答システム群を仮想受験者集団とみなす手法が提案されている\cite{tomikawa2024adaptive}．
この方法では，人間受験者の項目反応や特性が既知の項目データセットを利用せずに，仮想受験者を構築できるが，仮想受験者と人間受験者の反応傾向が一致する保証はない．
他方で，過去の項目群に対する人間受験者の反応データを用いて質問応答システムを訓練することで仮想受験者を構築する手法も提案されている\cite{uto2024question}．
この方法では，人間受験者の項目反応を模倣させやすい一方で，古典的な方法論と同様に，人間受験者の反応データの収集に多大なコストを要する．

これらの問題に対処できる仮想受験者の実現方法の一つとして，Benedetto et al.\cite{benedetto-etal-2024-using}は，LLMへのプロンプトとして，問題文と共に想定する受験者の特徴を与えることで，ゼロショットで受験者反応を模倣する手法を提案している．しかし，従来手法では，プロンプトに与える受験者の特徴が少数段階の能力レベルの指示のみに留まっており，細やかな知識状態の違いを模倣できていない．さらに，従来手法では，LLMによっては，適切に受験者レベルの調整ができないことを示している．これらはいずれも，プロンプトで与える受験者のプロフィールが荒いことに起因すると考えられる．

これらの問題を解決するために，本研究では，個別の受験者がどのような知識・技能を持っているかの詳細な定義を受験者プロフィールとして記述し，その情報をプロンプトとして与えることで，より精密に目標の受験者の反応を予測できる擬似受験者手法を提案する．具体的には，英語の読解力テストを対象に，ヨーロッパ言語共通参照枠（Common
European Framework of Reference for Languages: CEFR）に基づくA1からC2の各受験者レベルに対応する受験者プロフィールをCEFR Can-Do Statementを参考に設計して，プロンプトに含めることで，狙った知識・技能を持つ受験者の項目反応をより高精度に模倣することを目指す．

本研究では，実データ実験を通して提案モデルの有効性を示す．

\section{関連研究}

本研究では，Benedetto et al.\cite{benedetto-etal-2024-using}が提案したLLMに基づくゼロショット型の仮想受験者手法を基礎とするため，ここではこの手法を説明する．
この手法では，GPT-3.5やGPT-4などのLLMに対して特定の受験者レベル（1〜5の5段階）の受験者として振る舞うよう指示するプロンプトを与えることで，多肢選択問題への回答を生成させる．
LLMへの入力は目的とする受験者レベルと対象とする項目の問題文と選択肢のテキスト，および出力形式に関わる指示と読解問題の場合は読解パッセージから構成される．
出力指示では正当選択肢のインデックスに加え，項目の難易度レベルと回答プロセスの根拠説明（指定された受験者レベルが当該問題で辿る思考過程や誤解の説明）を出力させるようにしている．
% \textbf{Output (in JSON):}
% \begin{verbatim}
% {
%   "question_level": <question_level>,
%   "answer_explanation": "<answer_explanation>",
%   "index": <index>
% }
% \end{verbatim}

先行研究では，LLMが目的の受験者レベルに適した正答率分布を再現できるように，プロンプトエンジニアリングによる最適化を行っている．
具体的には，まず，5つの受験者レベルをそれぞれ指定したときの項目正答率をそれぞれ$a_1, a_2, a_3, a_4, a_5$（$a_i$はレベル$i$を指定した時の全項目にわたる正解率を意味する）とするとき，ベクトル$L = (a_1, a_2, a_3, a_4, a_5)$と理想的な正答率$I = (0.0, 0.25, 0.5, 0.75, 1.0)$ とのピアソン相関係数$\rho_{L,I}$を算出する．
さらに，正答率の非単調性に対するペナルティ$P$を以下の式で計算する．

\begin{equation}
P = \sum_{i=1}^{4} \sqrt{|a_{i+1} - a_i|} \cdot I(a_{i+1} < a_i)
\end{equation}

\noindent ここで，$I(a_{i+1} < a_i)$は$a_{i+1} < a_i$の場合に1，そうでなければ0となる指示関数である．
% 最終的な評価指標$M$は相関係数からペナルティを差し引いた値として次式で定義される．
そして，最終的な評価指標M を次式で計算し，これが最大化されるようにプロンプトの最適化を行なっている．

\begin{equation}
M = \rho_{L,I} - P
\end{equation}

この研究では，仮想受験者モデルの最終的な性能評価を，以下の二つの観点で行なっている．
\begin{enumerate}
\item 単調増加性：指定した受験者レベルの上昇に伴い正答率が単調増加するか．
\item 難易度依存性：同一の受験者レベルを指定した場合，問題の難易度増加で正答率が低下するか．
\end{enumerate}

実験では，GPT-3.5をベースモデルとして用いた場合には単調増加性が確認されたが，他のモデルでは単調増加性が損なわれることが確認された．
この要因の一つとして，従来手法では，仮想受験者の受験者レベルが1から5の数値のみで記述されており，受験者プロフィールが荒いことが考えられる．
レベルに対応する詳細な知識状態などが表現されていないため，狙った項目反応を再現できていない可能性がある．

\section{提案手法}

上記の問題を解決するために，本研究では，個別の受験者がどのような知識・技能を持っているかの詳細な定義を受験者プロフィールとして記述し，その情報をプロンプトとして与えることで，より精密に目標の受験者の反応を予測できる擬似受験者手法を提案する．提案手法では，英語の読解力テストを対象に，ヨーロッパ言語共通参照枠（Common European Framework of Reference for Languages: CEFR）に基づくA1 からC2の各受験者レベルに対応する受験者プロフィールをCEFR Can-Do Statement を参考に設計して，プロンプトに含める．
具体的には，CEFR Global Scale のA1からC2に対応したCan-Do Descriptorから，それぞれのレベルの受験者が運用可能な語彙や文法・語法の知識，読解時の認知的振る舞いなどの一覧を取得し，それを受験者の知識状態プロフィール（受験者プロフィールと呼ぶ）として文字列化して，LLMへの入力プロンプトに組み込む．さらにプロンプトでは，当該レベルよりも上位のレベルに相当する知識や振る舞いをしないように指示する．

\section{実験}

現在までには，実データを用いて従来手法の再現実験を行った．

実データとしては，英語読解問題の多肢選択式問題データセットとして様々な言語処理研究でベンチマークデータとして利用されるRACEデータセットを用いた．
RACEの各問題には3つの大まかな難易度レベル（middle, high, college）が割り当てられており，これをもとに難易度依存性の観点が分析できる．
本実験では，先行研究と同様に，層化サンプリングによって得られた，難易度レベルごとに50問ずつ，累計150問のデータを用いた．

この150問を，提案手法と同様にA1からC2までの6段階の受験者レベルをそれぞれ指定した従来手法に解かせ，その平均正答率を算出した．ベースとなるLLMとしては，先行研究で用いられたgpt-3.5-turbo-0613の提供が終了していたため，より新しいgpt-3.5-turbo-1106 を使用した．さらに，より新しいLLMとして，GPT-4o-miniを利用する実験も行った．

提案手法の実験結果を図\ref{fig:gpt-3-5}と\ref{fig:gpt-4o-mini}に示す．
図\ref{fig:gpt-3-5}および図\ref{fig:gpt-4o-mini}は，それぞれgpt-3.5-turbo-1106 とgpt-4o-mini の項目難易度別・受験者レベル別の正答率をサブプロットとして示している．
縦軸は正答率，横軸は指定した受験者レベル，各領域は項目難易度を表す．
図\ref{fig:gpt-3-5}より，難易度依存性は概ね確認できるが，指定した受験者レベルの増加と正答率に部分的な齟齬が生じており，単調増加性の破綻が再現された．これは，図2で示したgpt-4o-miniの場合により顕著であることがわかる．このことから，先行研究でも報告されているように，先行研究のプロンプトでは，必ずしも合理的な挙動を示さないことが確認された．
\begin{figure}[t]
\centering
\makebox[\columnwidth][c]{%
\includegraphics[width=\columnwidth]{img/gpt_3_5_1106/60_accuracy_per_difficulty_with_different_roleplayed_levels.eps}%
}
\vspace{2mm}
\caption{GPT-3.5-turbo-1106による難易度別正答率}
\label{fig:gpt-3-5}
\end{figure}

\begin{figure}[t]
\centering
\makebox[\columnwidth][c]{%
\includegraphics[width=\columnwidth]{img/gpt_4o_mini/68_accuracy_per_difficulty_with_different_roleplayed_levels.eps}%
}
\vspace{2mm}
\caption{GPT-4o-miniによる難易度別正答率}
\label{fig:gpt-4o-mini}
\end{figure}

\section{まとめ}

本研究では，LLMを用いた仮想受験者モデルの精度向上のため，CEFR Can-Do Statementに基づく詳細な受験者プロフィールをプロンプトに追加する手法を提案した．
現時点では，先行研究の再現実験を行い，従来手法が必ずしも指定した受験者レベルに応じた項目反応を保証しないことが確認された．今後は提案手法の実装を行い，従来手法との比較を通して有効性を評価していく．

{\small
\begin{thebibliography}{99}
% \bibitem{ueno2018irt} 植野真臣, 宮澤芳光, ``IRT-Based Adaptive Hints to Scaffold Learning in Programming", IEEE Transactions on Learning Technologies, Vol.11, No.4, pp.415--428, 2018.
% \bibitem{baker2004item} F. B. Baker, S.-H. Kim, \textit{Item Response Theory: Parameter Estimation Techniques}, CRC Press, 2004.
% \bibitem{van1996handbook} W. J. van der Linden, R. K. Hambleton, \textit{Handbook of Modern Item Response Theory}, Springer New York, 1996.
% \bibitem{kolen2014test} M. J. Kolen, R. L. Brennan, \textit{Test Equating, Scaling, and Linking}, Springer New York, 2014.
\bibitem{devlin2018bert} J. Devlin, M.-W. Chang, K. Lee, K. Toutanova, ``BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", arXiv preprint arXiv:1810.04805, 2018.
\bibitem{benedetto2023quantitative} L. Benedetto, ``A quantitative study of NLP approaches to question difficulty estimation", Proceedings of the International Conference on Artificial Intelligence in Education, pp.428--434, 2023.
% \bibitem{gao2019difficulty} Y. Gao, L. Bing, W. Chen, M. Lyu, I. King, ``Difficulty Controllable Generation of Reading Comprehension Questions", Proc. Twenty-Eighth International Joint Conf. Artificial Intelligence, pp.4968--4974, 2019.
% \bibitem{byrd2022predicting} M. Byrd, S. Srivastava, ``Predicting difficulty and discrimination of natural language questions", Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics, pp.119--130, 2022.
\bibitem{tomikawa2024adaptive} Y. Tomikawa, A. Suzuki, M. Uto, ``Adaptive Question–Answer Generation With Difficulty Control Using Item Response Theory and Pretrained Transformer Models", IEEE Transactions on Learning Technologies, Vol.17, pp.2186--2198, 2024.

\bibitem{uto2024question} M. Uto, Y. Tomikawa, A. Suzuki, ``Question Difficulty Prediction Based on Virtual Test-Takers and Item Response Theory", AIED'24: Workshop on Automatic Evaluation of Learning and Assessment Content, pp.1--11, 2024.
\bibitem{benedetto-etal-2024-using} L. Benedetto, G. Aradelli, A. Donvito, A. Lucchetti, A. Cappelli, P. Buttery, ``Using LLMs to simulate students' responses to exam questions", Findings of the Association for Computational Linguistics: EMNLP 2024, pp.11351--11368, 2024.
% \bibitem{lai2017race} G. Lai, Q. Xie, H. Liu, Y. Yang, E. Hovy, ``RACE: Large-scale ReAding Comprehension Dataset From Examinations", Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pp.785--794, 2017.
% \bibitem{liang2019new} Y. Liang, J. Li, J. Yin, ``A new multi-choice reading comprehension dataset for curriculum learning", Asian Conference on Machine Learning, pp.742--757, 2019.
% \bibitem{maeda2025field} H. Maeda, ``Field-testing multiple-choice questions with AI examinees: English grammar items", Educational and Psychological Measurement, Vol.85, No.2, pp.221--244, 2025.
% \bibitem{council2001} Council of Europe, \textit{Common European Framework of Reference for Languages: Learning, Teaching, Assessment}, Cambridge University Press, 2001.
\end{thebibliography}
}
\end{document}
